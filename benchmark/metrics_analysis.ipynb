{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import duckdb as ddb\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = \"s10\"\n",
    "with open(f\"{SF}_metrics.json\", \"r\") as file: \n",
    "    metrics = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_key_metrics_from_query_metrics(metric):\n",
    "    metrics_df = pd.DataFrame([ {\"run_number\":i, \"query_type\":key, **value}  for i,j in enumerate(metric) for key, value in j.items()])\n",
    "    metrics_df[\"time\"] = metrics_df[\"time\"]/1000\n",
    "    metrics_df[\"data_scanned\"] = metrics_df[\"data_scanned\"]/1000000\n",
    "    metrics_df[\"planning_time\"] = metrics_df[\"planning_time\"]/1000\n",
    "    metrics_df[\"execution_time\"] = metrics_df[\"execution_time\"]/1000\n",
    "    key_metrics_per_query = ddb.sql(\"\"\"\n",
    "        SELECT query_type, \n",
    "        avg(time):: DECIMAL(6,2) as avg_time, \n",
    "        avg(data_scanned):: DECIMAL(6,2) as avg_data_scanned, \n",
    "        avg(planning_time):: DECIMAL(6,2) as avg_planning_time, \n",
    "        avg(execution_time):: DECIMAL(6,2) as avg_execution_time,\n",
    "        stddev(time):: DECIMAL(6,2) as stddev_time \n",
    "        FROM\n",
    "        metrics_df\n",
    "        GROUP BY query_type\n",
    "        ORDER BY query_type asc\n",
    "        \"\"\").df()\n",
    "    \n",
    "    key_metrics_per_group = ddb.sql(\"\"\"\n",
    "        SELECT query_type[2:2] as query_group, run_number, \n",
    "        avg(time):: DECIMAL(6,2) as avg_time, \n",
    "        avg(data_scanned):: DECIMAL(6,2) as avg_data_scanned\n",
    "        FROM\n",
    "        metrics_df\n",
    "        GROUP BY query_type[2:2], run_number\n",
    "        ORDER BY query_type[2:2], run_number asc\n",
    "        \"\"\").df()\n",
    "    \n",
    "    key_metrics_arr = ddb.sql(\"\"\"\n",
    "        SELECT query_type, \n",
    "        list(time) as time, \n",
    "        list(data_scanned) as data_scanned\n",
    "        FROM\n",
    "        metrics_df\n",
    "        GROUP BY query_type\n",
    "        ORDER BY query_type asc\n",
    "        \"\"\").df()\n",
    "    return key_metrics_per_query, key_metrics_per_group, key_metrics_arr, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_full_hive = list(metrics[\"hive\"][\"queries_full\"].values())\n",
    "queries_incremental_hive = list(metrics[\"hive\"][\"queries_incremental\"].values())\n",
    "\n",
    "queries_full_iceberg = list(metrics[\"iceberg\"][\"queries_full\"].values())\n",
    "queries_incremental_iceberg = list(metrics[\"iceberg\"][\"queries_incremental\"].values())\n",
    "queries_rewrite_iceberg = list(metrics[\"iceberg\"][\"queries_rewrite\"].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_hive_full, per_group_hive_full, per_query_arr_hive_full, metrics_hive_full = calculate_key_metrics_from_query_metrics(queries_full_hive)\n",
    "per_query_hive_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_iceberg_full, per_group_iceberg_full, per_query_arr_iceberg_full, metrics_iceberg_full = calculate_key_metrics_from_query_metrics(queries_full_iceberg)\n",
    "per_query_iceberg_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_iceberg_full.sum(), per_query_hive_full.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting query times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "queries = per_query_hive_full[\"query_type\"]  # Labels for queries\n",
    "series1_mean = per_query_hive_full[\"avg_time\"]  # Random mean values for Series 1\n",
    "series2_mean = per_query_iceberg_full[\"avg_time\"]  # Random mean values for Series 2\n",
    "series1_stdev = per_query_hive_full[\"stddev_time\"]    # Random standard deviations for Series 1\n",
    "series2_stdev = per_query_iceberg_full[\"stddev_time\"]    # Random standard deviations for Series 2\n",
    "\n",
    "# Calculating 95% confidence intervals\n",
    "confidence_95 = 1.96  # 95% confidence level\n",
    "series1_conf = confidence_95 * series1_stdev\n",
    "series2_conf = confidence_95 * series2_stdev\n",
    "\n",
    "# Bar chart\n",
    "x = np.arange(len(queries))  # X-axis positions\n",
    "width = 0.35                 # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars with error bars for confidence intervals\n",
    "bars1 = ax.bar(x - width/2, series1_mean, width, yerr=series1_conf, label='Hive', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars2 = ax.bar(x + width/2, series2_mean, width, yerr=series2_conf, label='Iceberg', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "\n",
    "# Adding mean value labels on top of each bar\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Query', fontsize=12)\n",
    "ax.set_ylabel('Time (s)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(queries, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apllying t tests for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(per_query_arr_hive_full)):\n",
    "    mean = (per_query_arr_iceberg_full.iloc[i][\"time\"]/per_query_arr_hive_full.iloc[i][\"time\"]).mean()\n",
    "    result = pg.ttest(list(per_query_arr_iceberg_full.iloc[i][\"time\"]),list(per_query_arr_hive_full.iloc[i][\"time\"]),correction=True)\n",
    "    result = pg.ttest(list(per_query_arr_iceberg_full.iloc[i][\"time\"]),list(per_query_arr_hive_full.iloc[i][\"time\"]),correction=True)\n",
    "    results.append({\"query_type\": per_query_arr_hive_full.iloc[i][\"query_type\"], **result.iloc[0], \"mean\":mean})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 sample t-test for avg and raw values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_full[\"avg_time\"]/per_query_hive_full[\"avg_time\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_full[\"avg_time\"]/per_query_hive_full[\"avg_time\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_full[\"avg_data_scanned\"]/per_query_hive_full[\"avg_data_scanned\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_full[\"avg_data_scanned\"]/per_query_hive_full[\"avg_data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_full[\"time\"]/metrics_hive_full[\"time\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_full[\"time\"]/metrics_hive_full[\"time\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_full[\"data_scanned\"]/metrics_hive_full[\"data_scanned\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_full[\"data_scanned\"]/metrics_hive_full[\"data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_hive_incremental, per_group_hive_incremental, per_query_arr_hive_incremental, metrics_hive_incremental = calculate_key_metrics_from_query_metrics(queries_incremental_hive)\n",
    "per_query_hive_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_iceberg_incremental, per_group_iceberg_incremental, per_query_arr_iceberg_incremental, metrics_iceberg_incremental = calculate_key_metrics_from_query_metrics(queries_incremental_iceberg)\n",
    "per_query_iceberg_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_iceberg_rewrite, per_group_iceberg_rewrite, per_query_arr_iceberg_rewrite, metrics_iceberg_rewrite = calculate_key_metrics_from_query_metrics(queries_rewrite_iceberg)\n",
    "per_query_iceberg_rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_query_iceberg_incremental.sum(), per_query_hive_incremental.sum(), per_query_iceberg_rewrite.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Iceberg v Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "queries = per_query_hive_incremental[\"query_type\"]  # Labels for queries\n",
    "series1_mean = per_query_hive_incremental[\"avg_time\"]  # Random mean values for Series 1\n",
    "series2_mean = per_query_iceberg_incremental[\"avg_time\"]  # Random mean values for Series 2\n",
    "series1_stdev = per_query_hive_incremental[\"stddev_time\"]    # Random standard deviations for Series 1\n",
    "series2_stdev = per_query_iceberg_incremental[\"stddev_time\"]    # Random standard deviations for Series 2\n",
    "\n",
    "# Calculating 95% confidence intervals\n",
    "confidence_95 = 1.96  # 95% confidence level\n",
    "series1_conf = confidence_95 * series1_stdev\n",
    "series2_conf = confidence_95 * series2_stdev\n",
    "\n",
    "# Bar chart\n",
    "x = np.arange(len(queries))  # X-axis positions\n",
    "width = 0.35                 # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars with error bars for confidence intervals\n",
    "bars1 = ax.bar(x - width/2, series1_mean, width, yerr=series1_conf, label='Hive', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars2 = ax.bar(x + width/2, series2_mean, width, yerr=series2_conf, label='Iceberg', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "\n",
    "# Adding mean value labels on top of each bar\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Query', fontsize=12)\n",
    "ax.set_ylabel('Time (s)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(queries, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(per_query_arr_hive_incremental)):\n",
    "    mean = (per_query_arr_iceberg_incremental.iloc[i][\"time\"]/per_query_arr_hive_incremental.iloc[i][\"time\"]).mean()\n",
    "    result = pg.ttest(list(per_query_arr_iceberg_incremental.iloc[i][\"time\"]),list(per_query_arr_hive_incremental.iloc[i][\"time\"]),correction=True)\n",
    "    results.append({\"query_type\": per_query_arr_hive_incremental.iloc[i][\"query_type\"], \"T\": result[\"T\"][\"T-test\"], \"p-val\": result[\"p-val\"][\"T-test\"],\"mean\":mean})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_incremental[\"avg_time\"]/per_query_hive_incremental[\"avg_time\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_incremental[\"avg_time\"]/per_query_hive_incremental[\"avg_time\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_hive_incremental[\"avg_data_scanned\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_hive_incremental[\"avg_data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_incremental[\"time\"]/metrics_hive_incremental[\"time\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_incremental[\"time\"]/metrics_hive_incremental[\"time\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_incremental[\"data_scanned\"]/metrics_hive_incremental[\"data_scanned\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_incremental[\"data_scanned\"]/metrics_hive_incremental[\"data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_hive_incremental[\"avg_data_scanned\"]/per_query_hive_full[\"avg_data_scanned\"]).mean())\n",
    "pg.ttest(list(per_query_hive_incremental[\"avg_data_scanned\"]/per_query_hive_full[\"avg_data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_iceberg_full[\"avg_data_scanned\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_iceberg_full[\"avg_data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full X Incremental X Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "queries = per_query_hive_incremental[\"query_type\"]\n",
    "\n",
    "\n",
    "series1_mean = per_query_hive_incremental[\"avg_time\"]\n",
    "series1_stdev = per_query_hive_incremental[\"stddev_time\"] \n",
    "\n",
    "series2_mean = per_query_hive_full[\"avg_time\"]\n",
    "series2_stdev = per_query_hive_full[\"stddev_time\"]\n",
    "\n",
    "series3_mean = per_query_iceberg_incremental[\"avg_time\"] \n",
    "series3_stdev = per_query_iceberg_incremental[\"stddev_time\"]\n",
    "\n",
    "series4_mean = per_query_iceberg_full[\"avg_time\"] \n",
    "series4_stdev = per_query_iceberg_full[\"stddev_time\"]\n",
    "\n",
    "# Calculating 95% confidence intervals\n",
    "confidence_95 = 1.96  # 95% confidence level\n",
    "series1_conf = confidence_95 * series1_stdev\n",
    "series2_conf = confidence_95 * series2_stdev\n",
    "series3_conf = confidence_95 * series3_stdev\n",
    "series4_conf = confidence_95 * series4_stdev\n",
    "\n",
    "# Bar chart\n",
    "x = np.arange(len(queries))  # X-axis positions\n",
    "width = 0.2                 # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot bars with error bars for confidence intervals\n",
    "bars1 = ax.bar(x - 1.5*width, series1_mean, width, label='Hive_incremental', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars2 = ax.bar(x - 0.5*width, series2_mean, width, label='Hive_padrao', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars3 = ax.bar(x + 0.5*width, series3_mean, width, label='Iceberg_incremental', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars4 = ax.bar(x + 1.5*width, series4_mean, width, label='Iceberg_padrao', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "\n",
    "# Adding mean value labels on top of each bar\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars3:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars4:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Query', fontsize=12)\n",
    "ax.set_ylabel('Time (s)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(queries, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "queries = per_query_hive_incremental[\"query_type\"]\n",
    "\n",
    "\n",
    "series1_mean = per_query_iceberg_incremental[\"avg_time\"]\n",
    "series1_stdev = per_query_iceberg_incremental[\"stddev_time\"] \n",
    "\n",
    "series2_mean = per_query_iceberg_rewrite[\"avg_time\"]\n",
    "series2_stdev = per_query_iceberg_rewrite[\"stddev_time\"]\n",
    "\n",
    "series3_mean = per_query_iceberg_full[\"avg_time\"] \n",
    "series3_stdev = per_query_iceberg_full[\"stddev_time\"]\n",
    "\n",
    "\n",
    "# Calculating 95% confidence intervals\n",
    "confidence_95 = 1.96  # 95% confidence level\n",
    "series1_conf = confidence_95 * series1_stdev\n",
    "series2_conf = confidence_95 * series2_stdev\n",
    "series3_conf = confidence_95 * series3_stdev\n",
    "\n",
    "# Bar chart\n",
    "x = np.arange(len(queries))  # X-axis positions\n",
    "width = 0.25                 # Width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot bars with error bars for confidence intervals\n",
    "bars1 = ax.bar(x - width, series1_mean, width, label='Iceberg_incremental', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars2 = ax.bar(x , series2_mean, width, label='Iceberg_optimize', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "bars3 = ax.bar(x + width, series3_mean, width, label='Iceberg_padrao', capsize=5, alpha=0.8, error_kw={'alpha': 0.5})\n",
    "\n",
    "# Adding mean value labels on top of each bar\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars3:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() , f\"{bar.get_height():.1f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_xlabel('Query', fontsize=12)\n",
    "ax.set_ylabel('Time (s)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(queries, rotation=45, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_incremental[\"time\"]/metrics_iceberg_rewrite[\"time\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_incremental[\"time\"]/metrics_iceberg_rewrite[\"time\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_iceberg_rewrite[\"avg_data_scanned\"]).mean())\n",
    "pg.ttest(list(per_query_iceberg_incremental[\"avg_data_scanned\"]/per_query_iceberg_rewrite[\"avg_data_scanned\"]),1,correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((metrics_iceberg_incremental[\"data_scanned\"]/metrics_iceberg_rewrite[\"data_scanned\"]).mean())\n",
    "pg.ttest(list(metrics_iceberg_incremental[\"data_scanned\"]/metrics_iceberg_rewrite[\"data_scanned\"]),1,correction=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
